# TFIDF implementation-parallel computing

TFIDF (term frequency - inverse document frequency) is a numerical statistic that reflects how important a word is to a document in a corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.

Term Frequency = WordCount/docsize
Inverse Document Frequency = log(number of documents/ number of documents with word)

Serial Implementation - the serial code for calculating TFIDF of each word is implemented by using C. The code reads a document. I then calculates how many times a word appears in a document, the document size and how many documents are there containing the word. The code then calculates the TFIDF values for each word@document.

MPI and OpenMP implementation: The computation for calculating TFIDF is parallelized by using MPI and OpenMP.  MPI divides the computation between different nodes while OpenMP parallelizes the computation among processors on the same node.  The total time taken by serial code for 12000 words spread across 48 files was 3.200322 seconds while MPI and OpenMP implementation when ran on 3 nodes with 16 processors each for the same input data took around 0.1162063 seconds, thereby giving a speedup of 27.54.

Hadoop implementation: Code for calculating TFIDF is also implemented by using Map-Reduce in Hadoop. When ran with 3 slave nodes the code took 10.266 seconds for the above input. For 27500 words spread over 110 files, with 11 slave nodes Hadoop implementation took 14.911 seconds while the serial code for the same input took 16.517259 seconds. Therefore an improvement of 9.7% was achieved. Furthermore, with increasing input data and more slave nodes better speedup is expected.  

Spark Implementation: Code for calculating TFIDF is also implemented by using Map-Reduce in Spark. When ran with 11 slave nodes for input of 27500 words spread over 110 files, Spark implementation took 14.225 seconds while the serial code for the same input took 16.517259 seconds. Therefore an improvement of 13.88% was achieved. Furthermore, with increasing input data and more slave nodes better speedup is expected.

*Random data was generated by using wordsGenerator script written in shell scripting.
